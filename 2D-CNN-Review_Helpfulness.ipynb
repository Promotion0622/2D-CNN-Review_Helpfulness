{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d8abf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification, AutoModel, AutoTokenizer, TFGPT2Model, Trainer, TrainingArguments\n",
    "from sklearn.metrics import (accuracy_score, f1_score, precision_score, recall_score, mean_squared_error, roc_auc_score, r2_score, roc_curve, auc, mean_absolute_error, confusion_matrix, classification_report)\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.svm import SVC\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import (Embedding, Conv1D, LSTM, Dense, Conv2D, GlobalMaxPooling2D, GlobalMaxPooling1D, Reshape)\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from gensim.models import Word2Vec\n",
    "import urllib.request\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "os.environ[\"HF_HOME\"] = \"Nampromotion/KoGPT2-Review_Helpfulness\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b6cf992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # 한글과 공백을 제외한 모든 문자를 제거\n",
    "    return re.sub(\"[^가-힣\\s]\", \"\", text)\n",
    "\n",
    "def train_2d_cnn():\n",
    "    # 데이터 로드\n",
    "    train_data = pd.read_csv('/home/olga/NSJ/전처리/train.csv')\n",
    "    test_data = pd.read_csv('/home/olga/NSJ/전처리/test.csv')\n",
    "\n",
    "    train_data['review_text'] = train_data['review_text'].apply(preprocess_text)\n",
    "    test_data['review_text'] = test_data['review_text'].apply(preprocess_text)\n",
    "\n",
    "    # 텍스트 데이터 토큰화\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(train_data['review_text'])\n",
    "    X_train_tokenized = tokenizer.texts_to_sequences(train_data['review_text'])\n",
    "    X_test_tokenized = tokenizer.texts_to_sequences(test_data['review_text'])\n",
    "\n",
    "    # 단어 인덱스\n",
    "    word_index = tokenizer.word_index\n",
    "\n",
    "    # Word2Vec 모델 학습을 위한 토큰화된 문장 준비\n",
    "    sentences = [[word for word in str(document).split()] for document in train_data['review_text']]\n",
    "\n",
    "    # Word2Vec 모델 학습\n",
    "    word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, sg=0, workers=4)\n",
    "    word2vec_model.save(\"word2vec.model\")\n",
    "\n",
    "    # 임베딩 행렬 생성\n",
    "    vocab_size = len(word_index) + 1\n",
    "    embedding_matrix = np.zeros((vocab_size, 100))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            vector = word2vec_model.wv[word]\n",
    "            embedding_matrix[i] = vector\n",
    "        except KeyError:\n",
    "            continue\n",
    "\n",
    "    # 패딩\n",
    "    X_train_padded = pad_sequences(X_train_tokenized, padding='post')\n",
    "    X_test_padded = pad_sequences(X_test_tokenized, padding='post', maxlen=len(X_train_padded[0]))\n",
    "\n",
    "    # 모델 구축\n",
    "    model = Sequential([\n",
    "        Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False),\n",
    "        Reshape((X_train_padded.shape[1], 100, 1)),  # Conv2D 레이어에 맞게 형태 변경\n",
    "        Conv2D(128, (5, 5), activation='relu'),\n",
    "        GlobalMaxPooling2D(),\n",
    "        Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "    # 모델 컴파일\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    # 얼리스타핑\n",
    "    early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "    # 모델 훈련\n",
    "    history = model.fit(X_train_padded, train_data['review_usefulness'], epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "    # 모델 평가\n",
    "    y_pred = model.predict(X_test_padded)\n",
    "    y_pred_class = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "    # 학습 과정에서의 loss 및 accuracy 시각화\n",
    "    plt.figure(figsize=(12, 4))\n",
    "\n",
    "    # Loss 시각화\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss Evolution')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    # Accuracy 시각화\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy Evolution')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # 성능 지표\n",
    "    acc = accuracy_score(test_data['review_usefulness'], y_pred_class)\n",
    "    f1 = f1_score(test_data['review_usefulness'], y_pred_class)\n",
    "    precision = precision_score(test_data['review_usefulness'], y_pred_class)\n",
    "    recall = recall_score(test_data['review_usefulness'], y_pred_class)\n",
    "    roc_auc = roc_auc_score(test_data['review_usefulness'], y_pred)\n",
    "    mse = mean_squared_error(test_data['review_usefulness'], y_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(test_data['review_usefulness'], y_pred)\n",
    "    r2 = r2_score(test_data['review_usefulness'], y_pred)\n",
    "\n",
    "    print('Accuracy:', acc)\n",
    "    print('F1 Score:', f1)\n",
    "    print('Precision:', precision)\n",
    "    print('Recall:', recall)\n",
    "    print('ROC AUC:', roc_auc)\n",
    "    print('Mean Squared Error:', mse)\n",
    "    print('Root Mean Squared Error:', rmse)\n",
    "    print('Mean Absolute Error:', mae)\n",
    "    print('R2 Score:', r2)\n",
    "\n",
    "    return test_data['review_usefulness'].values, y_pred  # y_pred 값을 그대로 반환\n",
    "\n",
    "# 함수 실행\n",
    "#y_test_2d_cnn, y_pred_2d_cnn = train_2d_cnn()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75572c97",
   "metadata": {},
   "source": [
    "# Yelp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f724abf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_data = pd.read_csv('/home/olga/NSJ/Yelp/train.csv')\n",
    "test_data = pd.read_csv('/home/olga/NSJ/Yelp/test.csv')\n",
    "\n",
    "# 텍스트 데이터 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(train_data['text'])\n",
    "X_train_tokenized = tokenizer.texts_to_sequences(train_data['text'])\n",
    "X_test_tokenized = tokenizer.texts_to_sequences(test_data['text'])\n",
    "\n",
    "# 단어 인덱스\n",
    "word_index = tokenizer.word_index\n",
    "\n",
    "# Word2Vec 모델 학습을 위한 토큰화된 문장 준비\n",
    "sentences = [[word for word in str(document).split()] for document in train_data['review_text']]\n",
    "\n",
    "# Word2Vec 모델 학습\n",
    "word2vec_model = Word2Vec(sentences=sentences, vector_size=100, window=5, min_count=1, sg=0, workers=4)\n",
    "word2vec_model.save(\"word2vec.model\")\n",
    "\n",
    "# 임베딩 행렬 생성\n",
    "vocab_size = len(word_index) + 1\n",
    "embedding_matrix = np.zeros((vocab_size, 100))\n",
    "for word, i in word_index.items():\n",
    "    try:\n",
    "        vector = word2vec_model.wv[word]\n",
    "        embedding_matrix[i] = vector\n",
    "    except KeyError:\n",
    "        continue\n",
    "\n",
    "# 패딩\n",
    "X_train_padded = pad_sequences(X_train_tokenized, padding='post')\n",
    "X_test_padded = pad_sequences(X_test_tokenized, padding='post', maxlen=len(X_train_padded[0]))\n",
    "\n",
    "# 모델 구축\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 100, weights=[embedding_matrix], trainable=False),\n",
    "    Reshape((X_train_padded.shape[1], 100, 1)),  # Conv2D 레이어에 맞게 형태 변경\n",
    "    Conv2D(128, (5, 5), activation='relu'),\n",
    "    GlobalMaxPooling2D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 얼리스타핑\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# 모델 훈련\n",
    "history = model.fit(X_train_padded, train_data['useful'], epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# 모델 평가\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_class = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "# 성능 지표\n",
    "acc = accuracy_score(test_data['useful'], y_pred_class)\n",
    "f1 = f1_score(test_data['useful'], y_pred_class)\n",
    "precision = precision_score(test_data['useful'], y_pred_class)\n",
    "recall = recall_score(test_data['useful'], y_pred_class)\n",
    "roc_auc = roc_auc_score(test_data['useful'], y_pred)\n",
    "mse = mean_squared_error(test_data['useful'], y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(test_data['useful'], y_pred)\n",
    "r2 = r2_score(test_data['useful'], y_pred)\n",
    "\n",
    "print('Accuracy:', acc)\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('ROC AUC:', roc_auc)\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('R2 Score:', r2)\n",
    "\n",
    "# 에폭당 정확도 및 로스 그래프 그리기\n",
    "plt.figure(figsize=[8, 6])\n",
    "plt.plot(history.history['accuracy'], 'r', linewidth=3.0)\n",
    "plt.plot(history.history['val_accuracy'], 'b', linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Accuracy', fontsize=16)\n",
    "plt.title('Accuracy Curves', fontsize=16)\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=[8, 6])\n",
    "plt.plot(history.history['loss'], 'r', linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'], 'b', linewidth=3.0)\n",
    "plt.legend(['Training Loss', 'Validation Loss'], fontsize=18)\n",
    "plt.xlabel('Epochs', fontsize=16)\n",
    "plt.ylabel('Loss', fontsize=16)\n",
    "plt.title('Loss Curves', fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d2943b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0fb08d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bef81d7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b056c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80757ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1845a209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3f44f84",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "24cbe6a4",
   "metadata": {},
   "source": [
    "봉인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2bbcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로드\n",
    "train_data = pd.read_csv('/home/olga/NSJ/Yelp/train.csv')\n",
    "test_data = pd.read_csv('/home/olga/NSJ/Yelp/test.csv')\n",
    "\n",
    "X_train = train_data['text']\n",
    "y_train = train_data['useful']\n",
    "X_test = test_data['text']\n",
    "y_test = test_data['useful']\n",
    "\n",
    "# 텍스트 데이터 토큰화\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "X_train = tokenizer.texts_to_sequences(X_train)\n",
    "X_test = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# 패딩\n",
    "X_train = pad_sequences(X_train, padding='post')\n",
    "X_test = pad_sequences(X_test, padding='post', maxlen=len(X_train[0]))\n",
    "\n",
    "# Reshape the data\n",
    "X_train = np.expand_dims(X_train, axis=-1)\n",
    "X_test = np.expand_dims(X_test, axis=-1)\n",
    "\n",
    "# 모델 구축\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(vocab_size, 100, input_length=len(X_train[0])),\n",
    "    Reshape((len(X_train[0]), 100, 1)),  # Reshaping to 3D tensor\n",
    "    Conv2D(128, (5, 5), activation='relu'),\n",
    "    GlobalMaxPooling2D(),\n",
    "    Dense(1, activation='sigmoid')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# 얼리스타핑\n",
    "early_stopping = EarlyStopping(monitor='val_accuracy', patience=3, restore_best_weights=True)\n",
    "\n",
    "# 모델 훈련 및 학습 과정 저장\n",
    "history = model.fit(X_train, y_train, epochs=100, validation_split=0.2, callbacks=[early_stopping])\n",
    "\n",
    "# 모델 저장\n",
    "model.save('/home/olga/NSJ/model/2-D-CNN-Yelp')\n",
    "\n",
    "# 모델 평가\n",
    "best_model = load_model('/home/olga/NSJ/model/2-D-CNN-Yelp')\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_class = (y_pred > 0.5).astype(\"int32\")\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred_class)\n",
    "f1 = f1_score(y_test, y_pred_class)\n",
    "precision = precision_score(y_test, y_pred_class)\n",
    "recall = recall_score(y_test, y_pred_class)\n",
    "roc_auc = roc_auc_score(y_test, y_pred)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print('Accuracy:', acc)\n",
    "print('F1 Score:', f1)\n",
    "print('Precision:', precision)\n",
    "print('Recall:', recall)\n",
    "print('ROC AUC:', roc_auc)\n",
    "print('Mean Squared Error:', mse)\n",
    "print('Root Mean Squared Error:', rmse)\n",
    "print('Mean Absolute Error:', mae)\n",
    "print('R2 Score:', r2)\n",
    "\n",
    "# ROC 커브 그리기\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n",
    "\n",
    "# 에폭당 정확도 및 로스 그래프\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['accuracy'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_accuracy'],'b',linewidth=3.0)\n",
    "plt.legend(['Training Accuracy', 'Validation Accuracy'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Accuracy',fontsize=16)\n",
    "plt.title('Accuracy Curves',fontsize=16)\n",
    "\n",
    "plt.figure(figsize=[8,6])\n",
    "plt.plot(history.history['loss'],'r',linewidth=3.0)\n",
    "plt.plot(history.history['val_loss'],'b',linewidth=3.0)\n",
    "plt.legend(['Training loss', 'Validation Loss'],fontsize=18)\n",
    "plt.xlabel('Epochs ',fontsize=16)\n",
    "plt.ylabel('Loss',fontsize=16)\n",
    "plt.title('Loss Curves',fontsize=16)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
